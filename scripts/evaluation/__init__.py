"""
L4D2 Model Evaluation Package

This package provides tools for evaluating and validating L4D2 SourcePawn code
generated by AI models.

Modules:
    - benchmark_suite: Comprehensive benchmark suite for model testing
    - detect_antipatterns: Anti-pattern detection system
    - validate_generated_code: Multi-stage code validation gauntlet
    - compare_models: Model comparison utilities
    - compare_all_models: Multi-model comparison with Ollama support
    - openai_evals: OpenAI evaluation integration
"""

from pathlib import Path

# Version
__version__ = "1.0.0"

# Package root
PACKAGE_ROOT = Path(__file__).parent

# Lazy imports to avoid circular dependencies and heavy imports on package load
def get_anti_pattern_detector():
    """Get the AntiPatternDetector class."""
    from .detect_antipatterns import AntiPatternDetector
    return AntiPatternDetector

def get_benchmark_runner():
    """Get the BenchmarkRunner class."""
    from .benchmark_suite import BenchmarkRunner, BenchmarkEvaluator
    return BenchmarkRunner, BenchmarkEvaluator

def get_code_validator():
    """Get the code validation functions."""
    from .validate_generated_code import validate_code, validate_file
    return validate_code, validate_file

# Convenience function for quick anti-pattern scanning
def scan_antipatterns(code: str, file_path: str = "code.sp"):
    """
    Quick utility to scan code for anti-patterns.

    Args:
        code: SourcePawn code string
        file_path: Optional file path for reporting

    Returns:
        ScanResult with warnings and score
    """
    from .detect_antipatterns import AntiPatternDetector
    detector = AntiPatternDetector()
    return detector.scan(code, file_path)


def get_model_comparator():
    """Get the model comparison functions."""
    from .compare_all_models import (
        run_comparison,
        generate_markdown_report,
        OllamaInterface
    )
    return run_comparison, generate_markdown_report, OllamaInterface
