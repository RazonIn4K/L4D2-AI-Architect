# L4D2-AI-Architect: Qwen2.5-Coder Training Configuration
#
# Optimized for Vultr A100 (40GB VRAM) with Qwen2.5-Coder-7B-Instruct
#
# Why Qwen2.5-Coder:
#   - State-of-the-art code generation (2024-2025)
#   - Better instruction following than CodeLlama
#   - Excellent at structured output (SourcePawn)
#   - 128K context window (use 4K for training efficiency)
#
# Performance expectations:
#   - Higher quality code generation than Mistral
#   - Better at following complex prompts
#   - Improved understanding of game modding context

model:
  # Base model - Qwen2.5-Coder-7B-Instruct (4-bit quantized)
  name: "unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit"

  # Context window - reduced from 128K for training efficiency
  max_seq_length: 4096

  # Auto-detect dtype (will use bf16 on A100)
  dtype: null

  # 4-bit quantization for memory efficiency
  load_in_4bit: true

lora:
  # Higher LoRA rank for better code quality
  r: 64

  # Alpha = 2x rank
  lora_alpha: 128

  # No dropout - Unsloth optimized
  lora_dropout: 0

  # Target all attention and MLP layers
  # Qwen uses same architecture naming as Mistral
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

  bias: "none"

  # Unsloth gradient checkpointing
  use_gradient_checkpointing: "unsloth"

  # Standard LoRA
  use_rslora: false

training:
  # Training epochs
  num_train_epochs: 3

  # A100 40GB optimized batch size
  # Qwen is slightly larger than Mistral, may need to reduce if OOM
  per_device_train_batch_size: 6

  # Effective batch = 6 * 3 = 18
  gradient_accumulation_steps: 3

  # Lower learning rate for larger LoRA rank
  learning_rate: 1.5e-4
  weight_decay: 0.01
  warmup_steps: 50
  lr_scheduler_type: "cosine"

  # 8-bit AdamW optimizer
  optim: "adamw_8bit"

  # Use BF16 on A100
  fp16: false
  bf16: true

  # Logging frequency
  logging_steps: 10

  # Checkpoint every 100 steps
  save_steps: 100
  save_total_limit: 3

  # Reproducibility
  seed: 3407

  # Gradient clipping
  max_grad_norm: 1.0

data:
  # Training data - V15 dataset (2773 examples - latest)
  train_file: "l4d2_train_v15.jsonl"

  # Validation data
  val_file: "combined_val.jsonl"

  # Use all samples
  max_samples: null

output:
  # Output directory
  dir: "l4d2-qwen-v15-lora"

  # Hub settings
  push_to_hub: false
  hub_model_id: null

# A100-specific optimizations
advanced:
  # Flash Attention 2
  use_flash_attention_2: true

  # Multi-worker data loading
  dataloader_num_workers: 4

  # Pin memory for faster transfers
  dataloader_pin_memory: true

  # TF32 acceleration
  tf32: true

# Monitoring
monitoring:
  report_to: "tensorboard"
  logging_dir: "data/training_logs"
  evaluation_strategy: "steps"
  eval_steps: 100

# Expected performance on A100 40GB:
# - Training time for ~700 samples (3 epochs): ~2-2.5 hours
# - Memory usage: ~36-38GB peak
# - Better code quality than Mistral baseline
# - Estimated cost: ~$2.00-2.50
