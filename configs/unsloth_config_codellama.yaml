# L4D2-AI-Architect: CodeLlama-7B Training Configuration
#
# Optimized for Vultr A100 (40GB VRAM) with CodeLlama-7B
#
# Why CodeLlama:
#   - Code-focused architecture (trained on code)
#   - Strong syntax understanding
#   - Fill-in-the-middle (FIM) capability
#   - Good at structured languages like SourcePawn
#
# Performance expectations:
#   - Excellent syntax accuracy
#   - Strong pattern recognition for code structures
#   - May require more L4D2-specific examples vs general instruction models
#
# Batch size recommendations:
#   - A100 40GB: batch_size=8, gradient_accumulation=2 (this config)
#   - A40 48GB: batch_size=4, gradient_accumulation=4
#   - RTX 4090 24GB: batch_size=2, gradient_accumulation=8

model:
  # Base model - CodeLlama-7B (4-bit quantized)
  name: "unsloth/codellama-7b-bnb-4bit"

  # Context window
  max_seq_length: 4096

  # Auto-detect dtype (will use bf16 on A100)
  dtype: null

  # 4-bit quantization for memory efficiency
  load_in_4bit: true

lora:
  # LoRA rank - balanced for code generation
  r: 32

  # Alpha = 2x rank
  lora_alpha: 64

  # No dropout - Unsloth optimized
  lora_dropout: 0

  # Target all attention and MLP layers
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

  bias: "none"

  # Unsloth gradient checkpointing
  use_gradient_checkpointing: "unsloth"

  # Standard LoRA
  use_rslora: false

training:
  # Training epochs
  num_train_epochs: 3

  # A100 40GB optimized batch size
  per_device_train_batch_size: 8

  # Effective batch = 8 * 2 = 16
  gradient_accumulation_steps: 2

  # Learning rate
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 50
  lr_scheduler_type: "cosine"

  # 8-bit AdamW optimizer
  optim: "adamw_8bit"

  # Use BF16 on A100
  fp16: false
  bf16: true

  # Logging frequency
  logging_steps: 10

  # Checkpoint every 100 steps
  save_steps: 100
  save_total_limit: 3

  # Reproducibility
  seed: 3407

  # Gradient clipping
  max_grad_norm: 1.0

data:
  # Training data - V15 dataset (2773 examples - latest)
  train_file: "l4d2_train_v15.jsonl"

  # Validation data
  val_file: "combined_val.jsonl"

  # Use all samples
  max_samples: null

output:
  # Output directory
  dir: "l4d2-codellama-v15-lora"

  # Hub settings
  push_to_hub: false
  hub_model_id: null

# A100-specific optimizations
advanced:
  # Flash Attention 2
  use_flash_attention_2: true

  # Multi-worker data loading
  dataloader_num_workers: 4

  # Pin memory for faster transfers
  dataloader_pin_memory: true

  # TF32 acceleration
  tf32: true

# Monitoring
monitoring:
  report_to: "tensorboard"
  logging_dir: "data/training_logs"
  evaluation_strategy: "steps"
  eval_steps: 100

# Expected performance on A100 40GB:
# - Training time for ~1010 samples (3 epochs): ~1.5-2 hours
# - Memory usage: ~32-35GB peak
# - Strong syntax accuracy
# - Estimated cost: ~$1.50-2.00
